starting org.apache.spark.deploy.master.Master, logging to /home/cjwu/.spark/2.3.0/log/spark-8356573-org.apache.spark.deploy.master.Master-1-gra807.out
localhost: mv: cannot stat '/home/cjwu/.spark/2.3.0/log/spark-cjwu-org.apache.spark.deploy.worker.Worker-1-gra807.out.4': No such file or directory
localhost: mv: cannot move '/home/cjwu/.spark/2.3.0/log/spark-cjwu-org.apache.spark.deploy.worker.Worker-1-gra807.out.2' to '/home/cjwu/.spark/2.3.0/log/spark-cjwu-org.apache.spark.deploy.worker.Worker-1-gra807.out.3': No such file or directory
localhost: mv: cannot move '/home/cjwu/.spark/2.3.0/log/spark-cjwu-org.apache.spark.deploy.worker.Worker-1-gra807.out.1' to '/home/cjwu/.spark/2.3.0/log/spark-cjwu-org.apache.spark.deploy.worker.Worker-1-gra807.out.2': No such file or directory
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/cjwu/.spark/2.3.0/log/spark-cjwu-org.apache.spark.deploy.worker.Worker-1-gra807.out

write your message (for stdout) here line1
line2: 
Chr=3A

Verifying parameters ...

write your message (for stdout) here line1
line2: 
Chr=3A

Verifying parameters ...
spark.executor.memory:  20g
spark.driver.memory:  20g
spark.master:  spark://gra807.graham.sharcnet:7077
spark.driver.memoryOverhead:  None
spark.executor.memoryOverhead:  None
spark.cores.max:  24
spark.executor.memory:  20g
spark.driver.memory:  20g
spark.master:  spark://gra807.graham.sharcnet:7077
spark.driver.memoryOverhead:  None
spark.executor.memoryOverhead:  None
spark.cores.max:  24
infiles:
11w2013_t2_1.fasta

====================== mirLibSpark =========================
====================== app-20181015202916-0001 =================
Gap_Penalty :  -15
Max_Energy_cutoff :  -15
Max_Score_cutoff :  170
adapter :  TGGAATTCTCGGGTGCCAAGGAACTC
b_index_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/bowtie_index/
bowtie_index_suffix :  WHEAT_IWGSC
chromosomes :  3A
diffguide_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/diffguide_wheat.txt
gene_vs_pathway_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/ko_gene_vs_pathway.txt
genome_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/Genome/
input_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/input/
input_type :  c
known_non_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/ATH_TAIR10/TAIR10_ncRNA_CDS.gff
limit_len :  18
limit_m_freq :  100
limit_nbLoc :  15
limit_s_freq :  10
mcheck_param :  def
mirdup_limit :  0.98
mirdup_model :  thaliana.model
nbTargets :  100
output_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/output/
pathway_description_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/ko_pathway_description.txt
perform_KEGGpathways_enrichment_analysis :  no
perform_differnatial_analysis :  no
pre_flank :  10
pri_l_flank :  500
pri_r_flank :  200
project_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/
sc_appname :  mirLibSpark
sc_execcores :  24
sc_execmemory :  20g
sc_master :  local[*]
sc_mstrmemory :  20g
sc_partition :  16
target_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/mRNA_contigs_smaller.fasta
temperature :  25
============================================================

begin time: 2018-10-15 20:29:19.138362
--Processing of the library:  11w2013_t2_1.fasta
  Start of miRNA prediction...
infiles:
11w2013_t2_1.fasta

====================== mirLibSpark =========================
====================== app-20181015202916-0000 =================
Gap_Penalty :  -15
Max_Energy_cutoff :  -15
Max_Score_cutoff :  170
adapter :  TGGAATTCTCGGGTGCCAAGGAACTC
b_index_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/bowtie_index/
bowtie_index_suffix :  WHEAT_IWGSC
chromosomes :  3A
diffguide_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/diffguide_wheat.txt
gene_vs_pathway_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/ko_gene_vs_pathway.txt
genome_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/Genome/
input_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/input/
input_type :  c
known_non_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/ATH_TAIR10/TAIR10_ncRNA_CDS.gff
limit_len :  18
limit_m_freq :  100
limit_nbLoc :  15
limit_s_freq :  10
mcheck_param :  def
mirdup_limit :  0.98
mirdup_model :  thaliana.model
nbTargets :  100
output_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/output/
pathway_description_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/ko_pathway_description.txt
perform_KEGGpathways_enrichment_analysis :  no
perform_differnatial_analysis :  no
pre_flank :  10
pri_l_flank :  500
pri_r_flank :  200
project_path :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/
sc_appname :  mirLibSpark
sc_execcores :  24
sc_execmemory :  20g
sc_master :  local[*]
sc_mstrmemory :  20g
sc_partition :  16
target_file :  /home/cjwu/project/cjwu/gitRepo/mirLibSpark/dbs/WHEAT_IWGSC/mRNA_contigs_smaller.fasta
temperature :  25
============================================================

begin time: 2018-10-15 20:29:19.237028
--Processing of the library:  11w2013_t2_1.fasta
  Start of miRNA prediction...
NB distFile_rdd:  42375314
NB sr_short_rdd:  84171
NB dmask_rdd:  83970
NB mergebowtie_rdd:  21629
NB bowFrq_rdd:  21629
NB flat_rdd not distinct:  3873
NB pri_vld_rdd distinct (mircheck):  109
mergeChromosomesResults:  376
Traceback (most recent call last):
  File "/project/6001780/cjwu/gitRepo/mirLibSpark/workdir/../src/mirLibPipeline.py", line 206, in <module>
    if distFile_rdd.isEmpty():
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/pyspark.zip/pyspark/rdd.py", line 1392, in isEmpty
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/pyspark.zip/pyspark/rdd.py", line 1358, in take
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/pyspark.zip/pyspark/context.py", line 1001, in runJob
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py", line 1160, in __call__
Traceback (most recent call last):
  File "/project/6001780/cjwu/gitRepo/mirLibSpark/workdir/../src/mirLibPipeline.py", line 415, in <module>
    print('NB profile_rdd distinct: ', profile_rdd.groupByKey().count())
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/pyspark.zip/pyspark/rdd.py", line 1056, in count
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/pyspark.zip/pyspark/rdd.py", line 1047, in sum
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/pyspark.zip/pyspark/rdd.py", line 921, in fold
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/pyspark.zip/pyspark/rdd.py", line 824, in collect
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py", line 1160, in __call__
  File "/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/spark/2.3.0/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py", line 320, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job 12 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:837)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:835)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:835)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1838)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1751)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1924)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1923)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:572)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1988)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:938)
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)
	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)

